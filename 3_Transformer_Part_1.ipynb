{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Programming Assingnment3 - Implementation of Transformer Part 1\n","\n","안녕하세요 **AIKU 학회원 여러분**, 세 번째 과제랑 다음주에 나오는 네 번째 과제는 연계되는 과제로, Transformer 모델 전체를 직접 구현해보는 AIKU만의 Mini Term Project입니다. Transformer 모델은 NLP의 핵심 구조이므로, 이를 직접 구현하는 과정을 통해 언어 모델을 더 깊이 이해하고, 향후 Pytorch를 사용하여 모델을 구현할 때 큰 도움이 될 것입니다.\n","\n","모든 코드를 처음부터 구현하는 것은 매우 어려우므로, 각 함수와 클래스에 대한 설명을 docstring 형태로 제공해드렸습니다. 과제가 어려울 수 있지만, 여러분이 충분히 해낼 수 있도록 최선을 다해 도와드리겠습니다. 혹여나 오류가 있거나 과제 관련 문의사항이 있다면 주니어 무물딥 오픈카톡방 또는 과제 출제자인 부학회장한테 편하게 말해주세요! 이번주 및 다음주 과제를 성공적으로 수행한다면, 실력 향상은 장담할 수 있으니 부디 열심히 풀기를 바라겠습니다.\n","\n","이번 주에 여러분이 구현할 부분은 Transformer의 Encoder로, 이를 사용하여 IMDB 데이터셋에 대해 텍스트 분류 작업을 수행할 예정입니다.\n","\n","다음 자료를 참고하셔도 좋습니다:\n","[딥러닝을 이용한 자연어처리 입문 - Transformer](https://wikidocs.net/31379)"],"metadata":{"id":"GlIWlCu1cb5y"}},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/transformer-model-architecture.png)"],"metadata":{"id":"ka6H2paBcm85"}},{"cell_type":"markdown","source":["## Part 1. 준비"],"metadata":{"id":"Wm7BGAYLcy5d"}},{"cell_type":"markdown","source":["### 1-1. Pip install & Imports\n","필요한 패키지를 pip를 이용해서 설치합니다."],"metadata":{"id":"lS-c6WnCc0Bn"}},{"cell_type":"code","source":["!pip install torchdata torchtext spacy portalocker matplotlib seaborn wget"],"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Nps-lwDmZA1y","scrolled":true,"outputId":"b863bfa0-4815-4115-bc13-a7d088f2be13","execution":{"iopub.status.busy":"2024-07-30T05:53:16.446765Z","iopub.execute_input":"2024-07-30T05:53:16.447219Z","iopub.status.idle":"2024-07-30T05:53:32.296361Z","shell.execute_reply.started":"2024-07-30T05:53:16.447185Z","shell.execute_reply":"2024-07-30T05:53:32.295445Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: torchtext in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.5)\nCollecting portalocker\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nCollecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata) (1.26.18)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.32.3)\nRequirement already satisfied: torch>=2 in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.66.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.26.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.12.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.2.2)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (3.2.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (2024.5.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=a048426b92ff626dfbf3c2da904e77ddedf1817e1c81c07cdaa966922c5c61c2\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget, portalocker\nSuccessfully installed portalocker-2.10.1 wget-3.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":["필요한 라이브러리를 import하고, device를 GPU로 바꿔줍니다.\n","\n","다만, 코드를 구현하는 과정에서는 **CPU 사용**을 권장드립니다.\n","\n","GPU가 가장 많이 필요한 단계는 학습하는 과정인데 구현하는 시간동안 colab의 GPU를 다 써버리면 막상 학습할 때 필요한 GPU 자원을 쓸 수 없게 됩니다. 따라서 코드를 모두 구현하고 train 코드가 잘 돌아가는지 확인한 뒤에 colab 상단 런타임 메뉴에서 런타임 유형을 GPU로 바꾼 뒤에 실행하면 됩니다."],"metadata":{"id":"nmvfJSEudBmZ"}},{"cell_type":"code","source":["import os\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import nn, Tensor\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#device = 'cpu'\n","print('using device: ', device)"],"metadata":{"colab":{"background_save":true},"id":"9YZsPcGDW-mj","outputId":"f9251418-3a68-4f47-d99a-d8670ce55a1f","execution":{"iopub.status.busy":"2024-07-30T05:53:32.298348Z","iopub.execute_input":"2024-07-30T05:53:32.298639Z","iopub.status.idle":"2024-07-30T05:53:35.578072Z","shell.execute_reply.started":"2024-07-30T05:53:32.298611Z","shell.execute_reply":"2024-07-30T05:53:35.576858Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"using device:  cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Part 2. Transformer Architecture"],"metadata":{"id":"GWw6GyMX7BSi"}},{"cell_type":"markdown","source":["Transformer에 대해서 배우기 전에 기존의 seq2seq를 상기해봅시다. 기존의 seq2seq 모델은 encoder-decoder 구조로 구성되어져 있었습니다. 여기서 encoder는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, decoder는 이 벡터 표현을 통해서 출력 시퀀스를 만들어냈습니다. 하지만 이러한 구조는 encoder가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점이 있었고, 이를 보정하기 위해 attention이 사용되었습니다. 그런데 attention을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 attention만으로 encoder와 decoder를 만들어보면 어떨까요?\n","\n","![](https://wikidocs.net/images/page/31379/transformer1.PNG)\n","\n","Transformer는 RNN을 사용하지 않지만 기존의 seq2seq처럼 encoder에서 입력 시퀀스를 입력받고, decoder에서 출력 시퀀스를 출력하는 encoder-decoder 구조를 유지하고 있습니다. 이전 seq2seq 구조에서는 encoder와 decoder에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면 이번에는 encoder와 decoder라는 단위가 N개로 구성되는 구조입니다. Transformer를 제안한 논문에서는 encoder와 decoder의 개수를 각각 6개 사용하였습니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer2.PNG)\n","\n","위의 그림은 encoder와 decoder가 6개씩 존재하는 Transformer의 구조를 보여줍니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer4_final_final_final.PNG)\n","\n","위의 그림은 encoder로부터 정보를 전달받아 decoder가 출력 결과를 만들어내는 Transformer 구조를 보여줍니다. decoder는 마치 기존의 seq2seq 구조처럼 시작 심볼 $<sos>$를 입력으로 받아 종료 심볼 $<eos>$가 나올 때까지 연산을 진행합니다. 이는 RNN은 사용되지 않지만 여전히 encoder-decoder의 구조는 유지되고 있음을 보여줍니다.\n","\n","Transformer의 내부 구조를 조금씩 확대해가는 방식으로 Transformer를 이해해봅시다. 우선 encoder와 decoder의 구조를 이해하기 전에 Transformer의 입력에 대해서 이해해보겠습니다. Transformer의 encoder와 decoder는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받는데 이에 대해서 알아보기 위해 입력 부분을 확대해보겠습니다."],"metadata":{"id":"Gsw_H4nPyu0d"}},{"cell_type":"markdown","source":["## 2-1. Transformer Embedding\n"],"metadata":{"id":"feQfNhVIiYJq"}},{"cell_type":"markdown","source":["Transformer의 내부를 이해하기 전 우선 **Transformer의 입력**에 대해서 알아보겠습니다. RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보(position information)를 가질 수 있다는 점에 있었습니다.\n","\n","하지만 Transformer는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있습니다. Transformer는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 **포지셔널 인코딩(positional encoding)**이라고 합니다.\n","\n","Positional Encoding은 입력 문장 단어(토큰)들에 대한 위치정보를 인코딩하는 기술로, Transformer 모델의 입력 임베딩에 이 정보를 추가함으로써 단어의 상대적인 위치를 반영할 수 있습니다. 이렇게 함으로써 모델은 문장의 구조와 순서를 학습할 수 있게 됩니다.\n","\n","다음 코드는 Transformer에서 사용되는 대표적인 Positional Encoding 방법 중 하나인 Sinusodial Positional Encoding입니다. 이 방법은 고정된 함수로서 주기적인 값을 부여하여 위치정보를 인코딩합니다.\n","\n","Sinusodial Positional Encoding은 다음과 같은 수식을 사용하여 위치 인코딩 값을 계산합니다.\n","\n","$PE_{(pos,2i)}​ =sin(\\frac {pos}{10000^ {2i/d} model​}​ )$\n","\n","$PE_{(pos,2i+1)}​ =cos(\\frac {pos}{10000^ {2i/d} model​}​ )$\n","\n","여기서 $PE_{(pos, 2i)}$와 $PE_{(pos, 2i+1)}$는 Positional Encoding 행렬에서 $(pos, 2i)$와 $(pos, 2i+1)$ 위치에 해당하는 값을 의미하며, $pos$는 단어의 위치(position)를 나타내고, $i$는 인코딩 차원의 인덱스를 의미합니다. $d_{\\text{model}}$은 임베딩 차원의 크기를 나타냅니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer7.PNG)\n","\n","$pos$는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, $i$는 임베딩 벡터 내의 차원의 인덱스를 의미합니다. 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용합니다. 위의 수식에서 $(pos, 2i)$일 때는 사인 함수를 사용하고,  $(pos, 2i+1)$ 일 때는 코사인 함수를 사용하고 있음을 주목합시다.\n","\n","\n","이렇게 구해진 Positional Encoding 행렬은 입력 임베딩과 더해져서 최종 입력으로 들어가게 되며, 모델은 이를 활용하여 문장의 구조와 순서를 이해하고 학습합니다.\n","\n","\n","![](https://wikidocs.net/images/page/31379/transformer5_final_final.PNG)\n","\n","위의 그림은 입력으로 사용되는 임베딩 벡터들이 Transformer의 입력으로 사용되기 전에 positional encoding의 값이 더해지는 것을 보여줍니다. 임베딩 벡터가 encoder의 입력으로 사용되기 전 positional encoding값이 더해지는 과정을 시각화하면 아래와 같습니다.\n","\n","![](https://wikidocs.net/images/page/31379/transformer6_final.PNG)\n"],"metadata":{"id":"nVBa9llx7PQU"}},{"cell_type":"markdown","source":["Embedding vector를 위한 코드는 `TokenEmbedding` class로 이미 구현되어 있습니다. Positional encoding을 위해 `PositionalEncoding` class를 완성해주세요."],"metadata":{"id":"bseFO7ezPTfh"}},{"cell_type":"code","source":["class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    def forward(self, x):\n","        embeddings = self.embedding(x)\n","        embeddings = embeddings.to(x.device)\n","\n","        return embeddings"],"metadata":{"colab":{"background_save":true},"id":"e1sNGC1l0ye5","execution":{"iopub.status.busy":"2024-07-30T05:53:35.579311Z","iopub.execute_input":"2024-07-30T05:53:35.579694Z","iopub.status.idle":"2024-07-30T05:53:35.585445Z","shell.execute_reply.started":"2024-07-30T05:53:35.579668Z","shell.execute_reply":"2024-07-30T05:53:35.584462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, max_length, embedding_dim):\n","        super(PositionalEncoding, self).__init__()\n","        self.positional_encoding = self.create_positional_encoding(max_length, embedding_dim)\n","\n","    def get_angles(self, max_length, embedding_dim):\n","\n","        \"\"\"\n","        Calculates the angle values for positional encoding in a transformer model.\n","\n","        This function computes the sinusoidal positional encoding angles based on the specified maximum sequence length and the dimensionality of the embeddings.\n","        The positional encodings are used to add some information about the relative or absolute position of the tokens in the sequence.\n","        The formula used here ensures that each dimension of the positional encoding corresponds to a sinusoid of different frequencies and phases.\n","\n","        Parameters:\n","        max_length (int): The maximum length of the input sequences for which the positional encodings are to be generated.\n","        embedding_dim (int): The dimensionality of the embeddings. The number of dimensions should be even as the sinusoids are created for half this dimension size.\n","\n","        Returns:\n","        torch.Tensor: A tensor of shape (max_length, embedding_dim // 2) containing the positional encoding angles.\n","        \"\"\"\n","\n","        # Calculate positions for encoding\n","        position = torch.arange(0, max_length).unsqueeze(1)\n","\n","        # Calculate division terms for encoding frequencies\n","        div_term = torch.tensor([pow(10000, (2*i/embedding_dim)) for i in range(embedding_dim // 2)])\n","\n","        return position / div_term # (max_length, embedding_dim//2)\n","\n","\n","    def create_positional_encoding(self, max_length, embedding_dim):\n","        \"\"\"\n","        Generates the positional encoding matrix for transformer models.\n","        This function creates a positional encoding matrix using sinusoidal functions.\n","        These encodings provide the model with information about the position of the tokens in the sequence.\n","\n","        Parameters:\n","        max_length (int): The maximum length of the input sequences for which the positional encodings are to be generated.\n","        embedding_dim (int): The dimensionality of the embeddings.\n","\n","        Returns:\n","        positional_encoding (torch.Tensor): A tensor of shape (max_length, embedding_dim) containing the positional encoding matrix. Each row represents the positional encoding of a token in the sequence at different positions.\n","\n","        Notes:\n","        The encoding at each position is created by applying the sine function to even indices of the embedding dimensions and the cosine function to the odd indices.\n","        \"\"\"\n","\n","        angles = self.get_angles(max_length, embedding_dim)\n","        positional_encoding = torch.zeros(max_length, embedding_dim)\n","\n","        # positional_encoding\n","        # torch.sin(angles)와 torch.cos(angles)를 차원에 맞게 positional_encoding에 할당\n","        positional_encoding[:,::2] = torch.sin(angles)\n","        positional_encoding[:,1::2] = torch.cos(angles)\n","\n","        return positional_encoding\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Applies the positional encoding to the input tensor and returns the modified tensor.\n","        This method adjusts the positional encoding to the device of the input tensor `x` and adds the positional encoding to `x`.\n","        The positional encoding enhances the model's ability to understand the position of each element in the sequence. The addition of positional encodings to the input is a standard technique used in models like transformers to maintain the order of the input data.\n","\n","        Parameters:\n","        x (torch.Tensor): The input tensor to which positional encodings need to be added.\n","\n","        Returns:\n","        torch.Tensor: The tensor resulting from adding positional encodings to the input tensor `x`. This output tensor has the same shape as the input tensor.\n","\n","        Notes:\n","        The positional encodings are first transferred to the same device as `x` to ensure compatibility in operations.\n","        Only the required portion of the positional encoding matrix is used, corresponding to the actual sequence length of `x`.\n","        This portion is detached to prevent gradients from flowing into the positional encoding during backpropagation.\n","        \"\"\"\n","        # input x: (batch_size, sequence_length)\n","        # output: (batch_size, sequence_length, embedding_dim) for encode layer\n","        seq_len = x.size(1)\n","        positional_encoding = self.positional_encoding.to(x.device)\n","\n","        return positional_encoding[0:seq_len, :].detach() # 이녀석을 학습시켜선 안돼\n"],"metadata":{"colab":{"background_save":true},"id":"DubXWXYZ0y8F","execution":{"iopub.status.busy":"2024-07-30T05:53:35.586606Z","iopub.execute_input":"2024-07-30T05:53:35.586939Z","iopub.status.idle":"2024-07-30T05:53:35.601701Z","shell.execute_reply.started":"2024-07-30T05:53:35.586905Z","shell.execute_reply":"2024-07-30T05:53:35.600750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.tensor([[1, 2], [3, 4]])\n","t= t.unsqueeze(0).repeat(2, 1, 1)\n","z = torch.ones(2, 2)\n","print(t)\n","print(z)\n","print(t+z)"],"metadata":{"colab":{"background_save":true},"id":"_uxuT4FIJf5X","outputId":"8061aef7-2dd2-49d0-e85c-b6829be1a8cb","execution":{"iopub.status.busy":"2024-07-30T05:53:35.604150Z","iopub.execute_input":"2024-07-30T05:53:35.604432Z","iopub.status.idle":"2024-07-30T05:53:35.705045Z","shell.execute_reply.started":"2024-07-30T05:53:35.604408Z","shell.execute_reply":"2024-07-30T05:53:35.704175Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"tensor([[[1, 2],\n         [3, 4]],\n\n        [[1, 2],\n         [3, 4]]])\ntensor([[1., 1.],\n        [1., 1.]])\ntensor([[[2., 3.],\n         [4., 5.]],\n\n        [[2., 3.],\n         [4., 5.]]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":["제대로 구현했다면 다음과 같은 그래프가 생성되어야 합니다!\n","\n","![](https://wikidocs.net/images/page/31379/transformer8_final_ver.png)\n","\n","직접 구현한 PositionalEncoding class로 아래 코드를 이용해서 그래프를 생성해보세요."],"metadata":{"id":"YpAyuebqCpqB"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create an instance of the PositionalEncoding class\n","max_length = 50\n","embedding_dim = 128\n","\n","# Create an instance of the PositionalEncoding class\n","pos_encoding = PositionalEncoding(max_length, embedding_dim)\n","\n","# Convert the positional encoding tensor to a numpy array\n","pos_encoding_matrix = pos_encoding.positional_encoding.detach().numpy()\n","\n","# Plotting the heatmap of positional encodings\n","plt.pcolormesh(pos_encoding_matrix, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 128))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"metadata":{"colab":{"background_save":true},"id":"Dnhz5xI6Ci4o","outputId":"c6ee7579-0ab2-49e4-82cb-ec718751cb09","execution":{"iopub.status.busy":"2024-07-30T05:53:35.706092Z","iopub.execute_input":"2024-07-30T05:53:35.706378Z","iopub.status.idle":"2024-07-30T05:53:37.063215Z","shell.execute_reply.started":"2024-07-30T05:53:35.706355Z","shell.execute_reply":"2024-07-30T05:53:37.062349Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":["이제 전체 embedding값을 완성해봅시다"],"metadata":{"id":"12kjCj2VhCCR"}},{"cell_type":"code","source":["class TransformerEmbedding(nn.Module):\n","    \"\"\"\n","    Embedding layer for transformer models that combines token embeddings with positional encodings.\n","\n","    This class implements an embedding layer specific to transformer architectures, combining learned token embeddings with fixed (or learned) positional encodings.\n","    The token embeddings convert token indices into embeddings and the positional encodings provide additional context about the position of tokens within the sequence, which is crucial for models without recurrent structure.\n","\n","    Attributes:\n","    tok_emb (TokenEmbedding): A module to convert token indices into embeddings.\n","    pos_emb (PositionalEncoding): A module to generate positional encodings for tokens.\n","\n","    Parameters:\n","    vocab_size (int): The size of the vocabulary.\n","    embedding_dim (int): The dimensionality of the embeddings.\n","    max_len (int): The maximum length of the input sequences. Note that `max_len` should match the maximum length expected in the positional encoding module.\n","\n","    Methods:\n","    forward(x): Computes the embeddings by summing token and positional embeddings.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, embedding_dim, max_len):\n","\n","        super(TransformerEmbedding, self).__init__()\n","        self.tok_emb = TokenEmbedding(vocab_size, embedding_dim)\n","        self.pos_emb = PositionalEncoding(max_len, embedding_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Computes the embeddings for input indices by summing token and positional embeddings.\n","\n","        Parameters:\n","        x (torch.Tensor): The input tensor containing token indices.\n","\n","        Returns:\n","        torch.Tensor: The tensor containing combined embeddings which are the sum of token and positional embeddings.\n","        \"\"\"\n","        # input x: (batch_size, sequence_length)\n","        # output: (batch_size, sequence_length, embedding_dim) for encode layer\n","\n","        tok_emb = self.tok_emb(x)\n","        pos_emb = self.pos_emb(x)\n","\n","        return tok_emb + pos_emb"],"metadata":{"colab":{"background_save":true},"id":"Nj7yRCg4gUGa","execution":{"iopub.status.busy":"2024-07-30T05:53:37.064709Z","iopub.execute_input":"2024-07-30T05:53:37.065226Z","iopub.status.idle":"2024-07-30T05:53:37.072966Z","shell.execute_reply.started":"2024-07-30T05:53:37.065194Z","shell.execute_reply":"2024-07-30T05:53:37.071954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-2. Attention in Transformer\n"],"metadata":{"id":"VA-x5zNoizau"}},{"cell_type":"markdown","source":["### 2-2-1. Scaled Dot Product Attention\n","\n","![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/scale_dot_product_attention.png)\n","\n","Self-Attention 메커니즘 중 하나로, 입력 시퀀스 내의 단어들 간의 관계를 학습하는 데 사용되는 기술입니다. 입력 시퀀스의 모든 단어를 서로 다른 관련성 가중치로 가중 평균하여 표현하는 방법입니다.\n","\n","[계산 방법]\n","\n","1) 입력 시퀀스를 Query(Q), Key(K), Value(V)로 세 가지 선형 변환을 거칩니다. 이를 통해 각각의 단어들을 차원을 다르게하여 쿼리, 키, 밸류로 표현합니다.\n","\n","2) 쿼리(Q)와 키(K) 간의 유사도를 계산합니다. 일반적으로는 내적(dot-product)을 사용하여 유사도를 계산합니다.\n","\n","3) 유사도를 키(K)의 차원 수로 나누어, 스케일링(scaling)을 적용합니다. 스케일링은 유사도를 안정적으로 유지하기 위해 사용됩니다.\n","\n","4) 계산된 유사도를 소프트맥스(softmax) 함수를 통해 정규화합니다. 이로써 입력 시퀀스 내의 모든 단어들 간의 관련성 가중치를 얻을 수 있습니다.\n","\n","5) 정규화된 가중치와 키(K)에 대응하는 밸류(V)를 가중 평균하여 Self-Attention 값을 얻습니다. 이는 입력 시퀀스 내의 각 단어에 대해 중요도를 반영한 표현을 얻는 것을 의미합니다.\n","\n","Scaled Dot-Product Attention은 행렬 연산을 통해 병렬적으로 처리되기 때문에 다수의 단어들 간의 관계를 빠르게 계산할 수 있습니다. 이로 인해 Transformer 모델은 긴 시퀀스에 대해서도 비교적 높은 효율성을 유지할 수 있게 됩니다."],"metadata":{"id":"IJxuSYxbi7no"}},{"cell_type":"code","source":["class ScaledDotProductAttention(nn.Module):\n","    \"\"\"\n","    Implements the scaled dot-product attention mechanism.\n","\n","    This class provides a scaled dot-product attention mechanism as used in transformer models.\n","    It scales the dot products by the square root of the dimension of the attention keys, which helps in stabilizing gradients during backpropagation.\n","\n","\n","    Attributes:\n","    embedding_dim (int): The dimensionality of input embeddings.\n","    attention_dim (int): The dimensionality of the attention space, typically the same as `embedding_dim`.\n","    scale (torch.Tensor): The scaling factor for the attention scores, computed as the square root of `attention_dim`.\n","\n","    Parameters:\n","    embedding_dim (int): Dimensionality of the input embeddings.\n","    attention_dim (int): Size of the attention keys and values.\n","\n","    Methods:\n","    forward(query, key, value, mask=None): Computes the attention values based on the provided query, key, and value tensors. Optional masking can be applied.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim, attention_dim):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.attention_dim = attention_dim\n","\n","        # scaling factor 정의\n","        self.scale = torch.sqrt(torch.tensor(attention_dim, dtype=torch.float32))\n","\n","    def forward(self, query, key, value, mask=None):\n","        \"\"\"\n","        Computes the attention values using scaled dot-product attention mechanism.\n","\n","        Parameters:\n","        query (torch.Tensor): Query tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n","        key (torch.Tensor): Key tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n","        value (torch.Tensor): Value tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n","        mask (torch.Tensor, optional): The mask's elements are 0 where the attention should be masked, and 1 otherwise.\n","\n","        Returns:\n","        attention_value (torch.Tensor): The resulting tensor after applying attention, of shape (batch_size, num_heads, sequence_length, attention_dim).\n","\n","        Notes:\n","        - The mask is applied to the attention scores before softmax, setting masked positions to a large negative value to minimize their effect in the softmax step.\n","        \"\"\"\n","        # ScaledDotProductAttention 구현\n","        # attention_score shape (batch_size, num_heads, sequence_length, sequence_length)\n","        key = key.transpose(-2, -1)\n","        attention_score = torch.matmul(query, key) / self.scale\n","\n","        # masked_fill 함수\n","        # attention_score의 결과에서 mask가 0인 부분에 -1e10이라는 값을 채워넣는다.\n","        if mask is not None:\n","            attention_score = attention_score.masked_fill(mask == 0, -1e10)\n","\n","        # 가중치와 최종 attention value 구현\n","        attention_distribution = F.softmax(attention_score, dim=3)\n","        attention_value = torch.matmul(attention_distribution, value) # (batch_size, num_heads, sequence_length, attention_dim)\n","\n","        return attention_value"],"metadata":{"colab":{"background_save":true},"id":"R8X3P0lp02XQ","execution":{"iopub.status.busy":"2024-07-30T05:53:37.074219Z","iopub.execute_input":"2024-07-30T05:53:37.074507Z","iopub.status.idle":"2024-07-30T05:53:37.088040Z","shell.execute_reply.started":"2024-07-30T05:53:37.074485Z","shell.execute_reply":"2024-07-30T05:53:37.087308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-2-2. MultiHeadAttention\n","\n","\n","![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/multi_head_attention.png)\n","\n","마찬가지로 Self-Attention 메커니즘 중 하나로, 입력 시퀀스의 다양한 관점을 캡처하기 위해 여러 개의 Attention 헤드를 병렬로 사용하는 기법입니다. Self-Attention 레이어를 여러 개의 헤드로 나누고, 각 헤드에서 병렬로 Self-Attention을 수행하여 다양한 정보를 효과적으로 추출합니다.\n","\n","[계산 방법]\n","\n","1) 입력 시퀀스를 여러 개의 서로 다른 헤드로 분리합니다. 각 헤드는 별도의 Query(Q), Key(K), Value(V) 선형 변환을 적용합니다. 이를 통해 서로 다른 특성을 가진 Query, Key, Value를 추출할 수 있습니다.\n","\n","2) 각 헤드에서는 Scaled Dot-Product Attention을 사용하여 서로 다른 관점으로 입력 시퀀스의 단어들 간의 관계를 학습합니다. 각 헤드는 서로 다른 관점의 정보를 캡처하고, 다양한 종류의 패턴을 인식할 수 있게 됩니다.\n","\n","3) 계산된 Self-Attention 결과를 다시 하나의 행렬로 결합합니다. 이를 통해 서로 다른 헤드의 정보를 종합하여 최종 Self-Attention 결과를 얻을 수 있습니다.\n","\n","4) 병렬로 동작하는 여러 헤드를 가짐으로써, 모델은 다양한 관점에서 입력 시퀀스를 살펴볼 수 있고, 각 단어에 대해 다양한 특징을 추출하여 보다 풍부한 표현을 얻을 수 있게 됩니다."],"metadata":{"id":"c_JYdtf1JUW6"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Implements the Multi-Head Attention mechanism.\n","\n","    This class combines multiple attention heads to allow the model to jointly attend to information from different representation subspaces at different positions.\n","    The input is transformed into multiple heads, each with its own set of linear transformations for queries, keys, and values, followed by the scaled dot-product attention.\n","    The outputs of these heads are then concatenated and linearly transformed into the final output.\n","\n","    Attributes:\n","    num_heads (int): The number of attention heads.\n","    embedding_dim (int): The total dimension of the input embeddings.\n","    attention_dim (int): Dimension of each attention head.\n","\n","    scaled_dot_product_attention (ScaledDotProductAttention): The attention mechanism used in each head.\n","    Wq (nn.Linear): Linear transformation for query vectors.\n","    Wk (nn.Linear): Linear transformation for key vectors.\n","    Wv (nn.Linear): Linear transformation for value vectors.\n","    Wo (nn.Linear): Linear transformation that combines outputs from all attention heads.\n","\n","    Parameters:\n","    embedding_dim (int): Total dimensionality of the input embeddings.\n","    num_heads (int): Number of heads to split the embedding_dim into.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        # 변수 정의\n","        self.num_heads = num_heads\n","        self.embedding_dim = embedding_dim\n","        self.attention_dim = self.embedding_dim // self.num_heads\n","\n","        self.scaled_dot_product_attention = ScaledDotProductAttention(self.embedding_dim, self.attention_dim)\n","        self.Wq = nn.Linear(self.embedding_dim, self.embedding_dim)\n","        self.Wk = nn.Linear(self.embedding_dim, self.embedding_dim)\n","        self.Wv = nn.Linear(self.embedding_dim, self.embedding_dim)\n","        self.Wo = nn.Linear(self.embedding_dim, self.embedding_dim)\n","\n","    ######## 추가 ########\n","    def transpose_qkv(self, x):\n","      x = x.reshape(x.shape[0], x.shape[1], self.num_heads, self.attention_dim)\n","      x = x.transpose(1, 2)\n","      return x.to(x.device)\n","\n","    def forward(self, query, key, value, mask=None):\n","        \"\"\"\n","        Computes the output of the Multi-Head Attention layer.\n","\n","        Parameters:\n","        query (torch.Tensor): Query tensor of shape (batch_size, sequence_length, embedding_dim).\n","        key (torch.Tensor): Key tensor of shape (batch_size, sequence_length, embedding_dim).\n","        value (torch.Tensor): Value tensor of shape (batch_size, sequence_length, embedding_dim).\n","        mask (torch.Tensor, optional): Mask tensor (batch_size, 1, sequence_length, sequence_length) to exclude certain positions from attention.\n","\n","        Returns:\n","        output (torch.Tensor): The resulting tensor after applying multi-head attention, of shape (batch_size, sequence_length, embedding_dim).\n","        \"\"\"\n","        batch_size = query.size(0)\n","        sequence_length = query.size(1)\n","\n","        # MultiHeadAttention 계산 구현\n","        # Transforming the input and applying the attention mechanism for each head\n","        # query, key, value will have shape (batch_size, num_heads, sequence_length, attention_dim) after transformation\n","\n","        query = self.transpose_qkv(query)\n","        key = self.transpose_qkv(key)\n","        value = self.transpose_qkv(value)\n","\n","        # Calculating attention values\n","        attention_values = self.scaled_dot_product_attention(query, key, value, mask) # (batch_size, num_heads, sequence_length, attention_dim)\n","        attention_values = attention_values.transpose(1, 2).reshape(batch_size, sequence_length, self.num_heads * self.attention_dim)\n","\n","        # Combining attention outputs from all heads\n","        output = self.Wo(attention_values)\n","\n","        return output\n"],"metadata":{"colab":{"background_save":true},"id":"miuM4uVb05oZ","execution":{"iopub.status.busy":"2024-07-30T05:53:37.089303Z","iopub.execute_input":"2024-07-30T05:53:37.089628Z","iopub.status.idle":"2024-07-30T05:53:37.104020Z","shell.execute_reply.started":"2024-07-30T05:53:37.089596Z","shell.execute_reply":"2024-07-30T05:53:37.103164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-3. FeedForwardNeuralNetwork\n","\n","가장 간단한 형태의 FeedForward Network의 형태로 설명을 읽고 순서에 맞추어 구현해주시면 되겠습니다.\n","![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/feed-forward.png)\n","\n","Position-wise Feed-Forward Network은 각 위치(position)별로 독립적으로 적용되는 두 개의 선형 변환과 활성화 함수로 구성되는 네트워크입니다. 모델의 비선형성을 증가시키고, 입력 시퀀스의 각 위치에서 다양한 특징을 추출하는 데 도움이 됩니다.\n","\n","[설명]\n","\n","1) 입력 시퀀스의 각 위치별로, 먼저 하나의 선형 변환을 적용합니다. 이는 입력 시퀀스의 각 위치별로 입력 차원을 다른 차원으로 매핑하는 역할을 합니다.\n","\n","2) 활성화 함수로 주로 ReLU(Rectified Linear Unit)가 사용됩니다. 이 활성화 함수는 비선형성을 도입하여 모델이 더 복잡한 관계를 학습할 수 있도록 도와줍니다.\n","\n","3) 두 번째 선형 변환을 적용합니다. 이는 ReLU 활성화 함수를 통과한 결과를 다시 다른 차원으로 매핑하여 최종적인 출력 차원을 얻는 역할을 합니다.\n","\n","- PoswiseFeedForwardNet은 다음과 같은 수식으로 표현될 수 있습니다:\n","\n","- $PoswiseFeedForwardNet(x)=ReLU(xW_1​ +b_1​)W_2​ +b_2\n","​ $\n","\n","여기서\n","$x$는 입력 시퀀스의 각 위치에 대한 벡터를 나타내고, $W_1$과 $b_1$은 첫 번째 선형 변환의 가중치 행렬과 편향 벡터, $W_2$와 $b_2$는 두 번째 선형 변환의 가중치 행렬과 편향 벡터를 나타냅니다.\n","\n","모델이 입력 시퀀스의 각 위치에서 다양한 특징을 추출하고, 비선형성을 도입하여 더 풍부한 표현을 학습할 수 있도록 합니다."],"metadata":{"id":"7XpnzO0cMkVQ"}},{"cell_type":"code","source":["class FeedForwardNeuralNetwork(nn.Module):\n","    \"\"\"\n","    A simple feed-forward neural network module using PyTorch's nn.Module as a base class.\n","\n","    This class implements a feed-forward neural network with one hidden layer. It applies\n","    a linear transformation followed by a ReLU activation and dropout on the input,\n","    then another linear transformation to produce the output.\n","\n","    Attributes:\n","        W1 (nn.Linear): The first linear transformation layer from input dimension to hidden layer dimension.\n","        W2 (nn.Linear): The second linear transformation layer from hidden layer dimension back to the input dimension.\n","        relu (nn.ReLU): The ReLU activation function that introduces non-linearity after the first linear transformation.\n","        dropout (nn.Dropout): The dropout layer that randomly zeroes some of the elements of the input tensor with\n","            probability p during training, which helps prevent overfitting.\n","\n","    Parameters:\n","        embedding_dim (int): The dimensionality of the input and output embeddings.\n","        feed_forward_dim (int): The dimensionality of the hidden layer.\n","        drop_prob (float): The dropout probability for the dropout layer.\n","\n","    Methods:\n","    forward(inputs): Processes the input through the two linear layers and ReLU activation, producing the transformed output.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim, feed_forward_dim, drop_prob):\n","        super(FeedForwardNeuralNetwork, self).__init__()\n","        # 변수 정의\n","        self.W1 = nn.Linear(embedding_dim, feed_forward_dim)\n","        self.W2 = nn.Linear(feed_forward_dim, embedding_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Computes the output of the feed-forward neural network.\n","\n","        Parameters:\n","        inputs (torch.Tensor): The input tensor of shape (batch_size, sequence_length, embedding_dim).\n","\n","        Returns:\n","        outputs (torch.Tensor): The output tensor after processing through the feed-forward neural network, matching the shape of the input tensor (batch_size, sequence_length, embedding_dim).\n","        \"\"\"\n","        # FeedForwardNeuralNetwork 구현\n","        outputs = self.W2(self.relu(self.W1(inputs)))\n","\n","        return outputs"],"metadata":{"colab":{"background_save":true},"id":"1HXckFeA09p7","execution":{"iopub.status.busy":"2024-07-30T05:53:37.105239Z","iopub.execute_input":"2024-07-30T05:53:37.105648Z","iopub.status.idle":"2024-07-30T05:53:37.119216Z","shell.execute_reply.started":"2024-07-30T05:53:37.105624Z","shell.execute_reply":"2024-07-30T05:53:37.118391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래 `LayerNormalization` class는 이미 구현되어 있습니다!"],"metadata":{"id":"Qw4JYtPMVSP2"}},{"cell_type":"code","source":["class LayerNormalization(nn.Module):\n","    def __init__(self, num_features):\n","        super(LayerNormalization, self).__init__()\n","        self.layer_norm = nn.LayerNorm(num_features)\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        return x"],"metadata":{"colab":{"background_save":true},"id":"I8A1nAvY1IXU","execution":{"iopub.status.busy":"2024-07-30T05:53:37.120307Z","iopub.execute_input":"2024-07-30T05:53:37.120661Z","iopub.status.idle":"2024-07-30T05:53:37.133196Z","shell.execute_reply.started":"2024-07-30T05:53:37.120630Z","shell.execute_reply":"2024-07-30T05:53:37.132330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-4. Padding Mask\n","Padding Mask를 구하기 위한 함수 입니다.\n","\n","Transformer 모델은 입력 시퀀스의 길이가 가변적일 수 있기 때문에, 입력 시퀀스의 길이를 일정하게 맞추기 위해 패딩(Padding)을 사용합니다. 패딩은 특정 값을 사용하여 시퀀스의 길이를 일정하게 맞추는 기술로, 주로 0과 같은 값이 사용됩니다.\n","\n","하지만 입력 시퀀스의 길이가 다르면, 이로 인해 Masking이 필요합니다. Masking은 패딩된 부분에 대해서는 모델이 실제로 입력값으로 처리하지 않도록 막는 것을 의미합니다. 이렇게 함으로써 모델이 패딩된 부분을 무시하고 실제 입력값에만 집중하여 처리할 수 있게 됩니다.\n","\n","\"create_padding_mask_for_attention\" 함수는 이러한 패딩 부분에 대한 Mask를 구하는 함수로, 주어진 입력 시퀀스에서 패딩된 위치에 0으로 채워진 마스크를 생성합니다. 이렇게 생성된 마스크는 실제 입력값이 있는 위치는 1로 표시되고, 패딩된 위치는 0으로 표시됩니다.\n","\n","즉, 데이터로더에서 각 batch 별로 가장 긴 길이에 맞추어 이미 padding을 했습니다. mask는 각 input에서 padding된 위치를 알려주는 역할을 합니다.\n","\n","Ex) [55, 43, 102, 43, 0, 0, 0] ➡ [1, 1, 1, 1, 0, 0, 0]\n","\n","이렇게 생성된 마스크는 attention score를 구할 때, mask에서 0에 해당하는 부분을 굉장히 작은 음수값으로 설정해 사실상 attention score 연산에서 제외될 수 있도록 도와줍니다. 이 때, 사용하는 함수는 'masked_fill\" 함수입니다."],"metadata":{"id":"ybsEXaToWAMm"}},{"cell_type":"code","source":["def create_padding_mask_for_attention(batch: Tensor, pad_idx: int) -> Tensor:\n","    \"\"\"\n","    Creates a square binary padding mask for the attention mechanism, where the mask indicates\n","    whether each token in the input batch is a padding token or not. This mask can be used\n","    in attention mechanisms to prevent attention to padding tokens.\n","\n","    This function assumes that the input tensor `batch` is a 2D tensor representing sequences of\n","    tokens, where each element in the tensor is a token ID. The mask is constructed such that\n","    it has dimensions suitable for multi-head attention, adding two additional dimensions\n","    to cater to the heads and to square the mask.\n","\n","    Parameters:\n","    - batch (Tensor): A 2D tensor of shape (batch_size, sequence_length), where each entry is a token ID.\n","    - pad_idx (int): The token ID used to identify padding tokens in the batch.\n","\n","    Returns:\n","    - Tensor: A 4D binary mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n","              This tensor is placed on the same device as the input tensor `batch`.\n","              A value of `True` at a position indicates that the corresponding token\n","              is not a padding token and should be attended to, while `False` indicates\n","              it is a padding token and should not be attended to in the attention layers.\n","\n","    \"\"\"\n","    padding_mask = (batch != pad_idx)\n","    sequence_length = batch.size(1)\n","\n","    # padding mask를 생성하는 함수\n","    padding_mask = padding_mask.unsqueeze(1).repeat(1, sequence_length, 1)\n","    padding_mask = padding_mask.unsqueeze(1)\n","\n","    return padding_mask.to(batch.device)"],"metadata":{"colab":{"background_save":true},"id":"JmslkVSn1bow","execution":{"iopub.status.busy":"2024-07-30T05:53:37.134513Z","iopub.execute_input":"2024-07-30T05:53:37.135068Z","iopub.status.idle":"2024-07-30T05:53:37.144599Z","shell.execute_reply.started":"2024-07-30T05:53:37.135035Z","shell.execute_reply":"2024-07-30T05:53:37.143780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-5. Encoder Layer\n","**Encoder Layer를 직접 구현해봅니다!**\n","\n","앞 서 구현한 클래스를 바탕으로 Encoder Layer의 순서에 맞게 Encoder Layer를 구현해주시면 됩니다. 아래 이미지와 강의자료를 참고해주세요!\n","\n","Encoder Layer는 입력 시퀀스의 단어들을 인코딩하여 중간 표현을 생성하는 역할을 합니다.\n","\n","1) 입력 단어들에 대해 주변 단어들과의 관련성을 고려한 Self-Attention을 수행하여 풍부한 문맥 정보를 추출합니다.\n","\n","2) Feed-Forward Network를 통해 비선형성을 도입하고 다양한 특징을 추출합니다.\n","\n","3) Layer 간 잔차 연결과 Layer Normalization으로 Gradient Vanishing 문제를 완화하고, 모델 학습을 돕습니다.\n","\n","![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/encoder.png)\n"],"metadata":{"id":"wMt7QOfr-2IZ"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    \"\"\"\n","    Represents a single layer of a Transformer Encoder.\n","\n","    This layer applies multi-head self-attention followed by position-wise feed-forward networks on the input sequence.\n","    It incorporates dropout and layer normalization at both the attention and feed-forward stages to stabilize and regularize the training process.\n","\n","    Attributes:\n","        multi_head_attention (MultiHeadAttention): A multi-head attention mechanism.\n","        norm1 (LayerNormalization): Layer normalization applied after the attention mechanism.\n","        dropout1 (nn.Dropout): Dropout applied after the attention mechanism.\n","        feed_forward_neural_network (FeedForwardNeuralNetwork): A position-wise feed-forward neural network.\n","        norm2 (LayerNormalization): Layer normalization applied after the feed-forward network.\n","        dropout2 (nn.Dropout): Dropout applied after the feed-forward network.\n","\n","    Parameters:\n","        embedding_dim (int): The size of the input feature dimension.\n","        feed_forward_dim (int): The dimensionality of the feed-forward network's inner layer.\n","        num_heads (int): The number of heads in the multi-head attention mechanism.\n","        drop_prob (float): The dropout probability for the dropout layers.\n","\n","    Methods:\n","        forward(x, padding_mask): Defines the computation performed at every call. It takes inputs `x` and `padding_mask`, applies the attention and feed-forward mechanisms, and returns the transformed output.\n","    \"\"\"\n","    def __init__(self, embedding_dim, feed_forward_dim, num_heads, drop_prob):\n","        super(EncoderLayer, self).__init__()\n","        self.multi_head_attention = MultiHeadAttention(embedding_dim, num_heads)\n","        self.norm1 = LayerNormalization(embedding_dim)\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","\n","        self.feed_forward_neural_network = FeedForwardNeuralNetwork(embedding_dim, feed_forward_dim, drop_prob=drop_prob)\n","        self.norm2 = LayerNormalization(embedding_dim)\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","\n","\n","    def forward(self, x, padding_mask):\n","        \"\"\"\n","      Process the input tensor through the encoder layer to refine the representations with attention and feed-forward networks.\n","\n","      Parameters:\n","          x (Tensor): The input tensor to the encoder layer, which should have the shape\n","                      (batch_size, sequence_length, embedding_dim), representing a batch of sequences with embeddings.\n","          padding_mask (Tensor): The padding mask tensor, shaped (batch_size, 1, sequence_length, sequence_length),\n","                                used in the multi-head attention to prevent attention to padding positions within the sequence.\n","\n","      Returns:\n","          Tensor: The output tensor from the encoder layer, maintaining the shape (batch_size, sequence_length, embedding_dim).\n","                  This output contains the refined embeddings after applying multi-head attention, dropout, and normalization steps.\n","      \"\"\"\n","\n","        # encoder layer\n","        input_x = x\n","        x = self.multi_head_attention(x, x, x, padding_mask)\n","        x = self.dropout1(x)\n","        x += input_x\n","        x = self.norm1(x)\n","\n","        attention_x = x\n","        x = self.feed_forward_neural_network(x)\n","        x = self.dropout2(x)\n","        x += attention_x\n","        x = self.norm2(x)\n","\n","        return x.to(x.device)"],"metadata":{"colab":{"background_save":true},"id":"TZCLBbOf2IvD","execution":{"iopub.status.busy":"2024-07-30T05:53:37.145682Z","iopub.execute_input":"2024-07-30T05:53:37.145943Z","iopub.status.idle":"2024-07-30T05:53:37.158874Z","shell.execute_reply.started":"2024-07-30T05:53:37.145920Z","shell.execute_reply":"2024-07-30T05:53:37.158093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Encoder Layer를 구현했다면 전체 Transformer Encoder의 구조를 구현해봅시다.**\n","\n","해당 Encoder가 전체 input부터 output까지 출력하는 전체 구조입니다. 역시나 앞 서 구현한 클래스들을 바탕으로 구현해주시면 되고 Transformer Encoder에는 Encoder Layer(Block)이 여러 개(논문 기준 6개)가 포함된다고 설명드렸습니다. 이 점에 유의해서 구현해주세요!\n","\n","1) 입력에 대한 Position 값을 구하기: 각 단어의 상대적인 위치를 나타내는 Positional Encoding을 계산합니다.\n","\n","2) Input Embedding과 Position Embedding 더하기: 입력 시퀀스의 단어들을 임베딩하여 벡터로 표현한 후, Positional Encoding을 더합니다. 이를 통해 입력 시퀀스의 단어들은 고유한 위치 정보를 가진 임베딩으로 변환됩니다.\n","\n","3) 입력에 대한 Attention Pad Mask 구하기: Self-Attention 레이어에서 패딩 부분에 대한 마스크를 생성합니다. 이렇게 함으로써 모델이 패딩 부분을 무시하고, 실제 입력에만 집중할 수 있도록 돕습니다.\n","\n","4) for 루프를 돌며 각 layer를 실행하기: 여러 개의 EncoderLayer로 구성된 스택을 순차적으로 거칩니다. 각 EncoderLayer는 입력 시퀀스에 대한 인코딩을 수행하고, 다음 EncoderLayer로 전달하기 위해 중간 결과를 출력합니다.\n","\n","5) layer의 입력은 이전 layer의 출력 값: 스택의 첫 번째 EncoderLayer를 거칠 때는 이전 layer가 없으므로, Input Embedding과 Position Embedding의 결과가 첫 번째 EncoderLayer의 입력으로 사용됩니다. 이후의 EncoderLayer들은 이전 layer의 출력 값을 입력으로 받아 처리합니다.\n","\n","이렇게 입력 시퀀스에 대한 인코딩을 위해 Positional Encoding, Self-Attention, Residual Connection 등의 기법을 사용하여 입력 정보를 풍부하게 표현합니다. 이 과정을 여러 번 쌓아 올려서 Encoder를 형성하며, 최종적으로 입력 시퀀스의 문맥 정보를 잘 반영한 중간 표현을 얻습니다."],"metadata":{"id":"nk2dSn7d-2IZ"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    \"\"\"\n","    A Transformer encoder module for processing sequences using multiple encoder layers.\n","\n","    This class is a PyTorch module that utilizes a stack of transformer encoder layers to process\n","    input sequences. It first applies positional embeddings using a TransformerEmbedding layer\n","    before passing the data through multiple transformer encoder layers.\n","\n","    Attributes:\n","        transformer_embedding (TransformerEmbedding): An embedding layer that adds position\n","            embeddings to the token embeddings.\n","        encoder_layers (nn.ModuleList): A list of encoder layers (instances of EncoderLayer)\n","            used to process the input data sequentially.\n","\n","    Args:\n","        vocab_size (int): The size of the vocabulary (number of unique tokens).\n","        sequence_length (int): The sequence length of input sequences.\n","        embedding_dim (int): The dimensionality of token embeddings.\n","        feed_forward_dim (int): The dimensionality of the feed-forward networks in the encoder layers.\n","        num_heads (int): The number of attention heads in each encoder layer.\n","        num_layers (int): The number of encoder layers in the module.\n","\n","    Methods:\n","        forward(x, padding_mask):\n","            Processes input data through the transformer embedding and each encoder layer sequentially.\n","\n","            Args:\n","                x (Tensor): The input tensor containing token indices, shape [batch_size, sequence_length].\n","                padding_mask (Tensor): The padding mask for the input tensor, shape [batch_size, sequence_length].\n","\n","            Returns:\n","                Tensor: The output tensor from the final encoder layer, shape [batch_size, sequence_length, embedding_dim].\n","    \"\"\"\n","    def __init__(self, vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob):\n","        super(Encoder, self).__init__()\n","\n","        # 변수 정의\n","        self.transformer_embedding = TransformerEmbedding(vocab_size, embedding_dim, sequence_length)\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim, feed_forward_dim, num_heads, drop_prob) for _ in range(num_layers)])\n","\n","    def forward(self, x, padding_mask):\n","        x = self.transformer_embedding(x)\n","\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x, padding_mask)\n","\n","        return x"],"metadata":{"colab":{"background_save":true},"id":"TvfIQzx42JRS","execution":{"iopub.status.busy":"2024-07-30T05:53:37.161998Z","iopub.execute_input":"2024-07-30T05:53:37.162342Z","iopub.status.idle":"2024-07-30T05:53:37.172883Z","shell.execute_reply.started":"2024-07-30T05:53:37.162319Z","shell.execute_reply":"2024-07-30T05:53:37.172054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-6. Encoder를 활용해 Classifier 구현\n","\n","이번에는 IMDB 데이터셋을 사용하여 영화 리뷰의 감성을 분석해보려 합니다. Transformer Transformer Encoder로 input sentence를 Encoding한 뒤 해당 Encoding을 가장 간단한 classifier에 통과시켜 해당 리뷰가 긍정(1)인지 부정(0)인지를 분류하는 Classification 모델입니다."],"metadata":{"id":"T0WnhDbGgZkW"}},{"cell_type":"code","source":["class Classifier(nn.Module):\n","    def __init__(self, vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob):\n","        super(Classifier, self).__init__()\n","        self.encoder = Encoder(vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob)\n","\n","        # classifier를 구현\n","        # Encoder의 output의 shape: [batch_size, sequence_length, embedding_dim]\n","        # 구현한 classifier는 아래에 있는 IMDB 데이터셋에 대한 이진 분류 (binary classification)에 활용\n","\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(drop_prob),\n","            nn.Linear(embedding_dim,  embedding_dim),\n","            nn.ReLU(),\n","            nn.Dropout(drop_prob),\n","            nn.Linear(embedding_dim,  1),\n","        )\n","\n","\n","    def forward(self, x, padding_mask):\n","        # Output of encoder shape expected: [batch_size, sequence_length, embedding_dim]\n","        x = self.encoder(x, padding_mask)\n","        x = x.mean(dim=1)\n","        # classifier를 이용해서 output을 계산\n","        output = self.classifier(x).squeeze(1)\n","        return output.to(x.device)"],"metadata":{"colab":{"background_save":true},"id":"T2pGbGpb-2Ia","execution":{"iopub.status.busy":"2024-07-30T05:53:37.173854Z","iopub.execute_input":"2024-07-30T05:53:37.175364Z","iopub.status.idle":"2024-07-30T05:53:37.186491Z","shell.execute_reply.started":"2024-07-30T05:53:37.175340Z","shell.execute_reply":"2024-07-30T05:53:37.185726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-7. Dataset 준비\n","\n","아래 코드는 torchtext 라이브러리를 활용해 IMDB 영화 리뷰 데이터셋을 준비합니다. Tokenizer를 사용해 텍스트를 토큰으로 분리하고, IMDB 영화 리뷰 데이터셋의 training set을 사용하여 어휘를 구축합니다. 어휘에는 특수 토큰으로 `<unk>`(알 수 없는 토큰)와 `<pad>`(패딩 토큰)가 포함됩니다. Training 및 test 데이터셋은 `random_split` 함수를 사용하여 분할되며, 이는 데이터셋 길이 관련 에러를 방지하기 위해 사용됩니다. 추가적으로, 텍스트를 수치화된 토큰 ID로 변환하는 함수와 라벨을 인코딩하는 함수를 정의하여, 이를 통해 데이터를 모델 학습에 적합한 형태로 전처리합니다. 추가적으로 구현할 부분 없이 그대로 사용하시면 됩니다."],"metadata":{"id":"eanedfxniK5l"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","from torchtext.datasets import IMDB\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.optim as optim\n","\n","# Tokenizer\n","tokenizer = get_tokenizer('basic_english')\n","\n","# Building the vocabulary\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","# Load the IMDb dataset\n","train_iter, test_iter = IMDB(split=('train', 'test'))\n","torch.manual_seed(42)\n","\n","# random_split을 활용하는 이유는 'TypeError: MapperIterDataPipe instance doesn't have valid length' 를 방지하기 위함이고 특별한 이유는 따로 없습니다.\n","train_iter, _ = random_split(list(train_iter), [24000, 1000])\n","test_iter, _ = random_split(list(test_iter), [24000, 1000])\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])\n","UNK_IDX, PAD_IDX = 0, 1\n","MAX_SEQ_LENGTH = 512\n","\n","# Encoding text function\n","text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)[:MAX_SEQ_LENGTH]]\n","label_pipeline = lambda x: int(x) - 1"],"metadata":{"colab":{"background_save":true},"id":"t5FkbmaD5ChO","execution":{"iopub.status.busy":"2024-07-30T05:53:37.187489Z","iopub.execute_input":"2024-07-30T05:53:37.187766Z","iopub.status.idle":"2024-07-30T05:54:15.225258Z","shell.execute_reply.started":"2024-07-30T05:53:37.187743Z","shell.execute_reply":"2024-07-30T05:54:15.224364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래 코드는 `collate_batch` 함수를 정의하고 `DataLoader`를 만드는 과정을 포함하고 있습니다.\n","\n","collate_fn은 DataLoader에서 데이터를 불러올 때 데이터 전처리를 할 수 있도록 만들어줍니다. 여기서는 데이터 전처리에 collate_batch 함수를 사용합니다. 이 합수는 label에는 -1을 적용하고, text는 tokenize 후 index로 변환하고, padding을 추가합니다. 이후 label과 text를 tensor 형태로 return 합니다."],"metadata":{"id":"HHpcPzSZkyLX"}},{"cell_type":"code","source":["# Data loader collation function with padding and truncation\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for (_label, _text) in batch:\n","        label_list.append(float(label_pipeline(_label)))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","\n","    text_list = nn.utils.rnn.pad_sequence(text_list, padding_value=vocab['<pad>'], batch_first=True)\n","    text_list = text_list[:, :MAX_SEQ_LENGTH]\n","\n","    return torch.tensor(label_list, dtype=torch.float), text_list\n","\n","\n","# Create DataLoader for train and test\n","train_dataloader = DataLoader(train_iter, batch_size=64, drop_last=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_iter, batch_size=64, collate_fn=collate_batch)"],"metadata":{"colab":{"background_save":true},"id":"WEqJ1c1RiAxg","execution":{"iopub.status.busy":"2024-07-30T05:54:15.226468Z","iopub.execute_input":"2024-07-30T05:54:15.226970Z","iopub.status.idle":"2024-07-30T05:54:15.235923Z","shell.execute_reply.started":"2024-07-30T05:54:15.226943Z","shell.execute_reply":"2024-07-30T05:54:15.234918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2-8. Training 및 Evaluation\n"],"metadata":{"id":"_9QEhZVElAPQ"}},{"cell_type":"markdown","source":["이제 직접 학습을 해봅시다. `Classifier(vocab_size=len(vocab), sequence_length=MAX_SEQ_LENGTH` 이 부분 제외하고 자유롭게 수정하셔도 됩니다. 원하시면 learning rate scheduler를 사용하셔도 되고 다른 training technique을 사용하셔도 됩니다. 대신 마지막 코드에서 test set에 대한 evaluation 결과 **accuracy가 0.8 이상 나와야 됩니다.**"],"metadata":{"id":"eNVgm8rhnaji"}},{"cell_type":"code","source":["from IPython.display import display\n","from tqdm import tqdm, tqdm_notebook, trange\n","import wget\n","\n","\n","# embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob 값 정의\n","# vocab_size=len(vocab)과 sequence_length=MAX_SEQ_LENGTH 부분은 수정하지 말것.\n","model = Classifier(vocab_size=len(vocab), sequence_length=MAX_SEQ_LENGTH, embedding_dim=512,\n","                   feed_forward_dim=1024, num_heads=8, num_layers=6,\n","                   drop_prob=0.1).to(device) #논문기반 설정\n","\n","class LRScheduler: # 임의로 구현하니 성능 구려서 논문대로 lr 설정하기 위한 scheduler\n","    def __init__(self, optimizer, model_dim, warmup_steps=4000):\n","        self.optimizer = optimizer\n","        self.model_dim = model_dim\n","        self.warmup_steps = warmup_steps\n","        self.step_num = 0\n","\n","    def step(self):\n","        self.step_num += 1\n","        lr = self.compute_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","    def compute_lr(self):\n","        temp1 = self.step_num**(-0.5)\n","        temp2 = self.step_num * (self.warmup_steps**(-1.5))\n","        return (self.model_dim ** (-0.5)) * min(temp1, temp2)\n","\n","def train(dataloader):\n","    model.train()\n","    total_loss = 0\n","    # Wrap the dataloader with tqdm for a progress bar\n","    progress_bar = tqdm(dataloader, desc=\"Training\", leave=True)\n","    for label, text in progress_bar:\n","        label, text = label.to(device), text.to(device)\n","        optimizer.zero_grad()\n","\n","        # Assuming create_padding_mask_for_attention is defined elsewhere\n","        src_padding_mask = create_padding_mask_for_attention(text, PAD_IDX)\n","\n","        output = model(text, src_padding_mask).to(device)\n","        loss = criterion(output, label)\n","        loss.backward()\n","\n","        # Gradient clipping은 사용해도 되고 안해도 된다\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        # Update progress bar with the current loss\n","        progress_bar.set_postfix(loss=loss.item())\n","\n","    return total_loss / len(dataloader)\n","\n","# Loss function과 optimizer 정의\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9) # 논문기반\n","scheduler = LRScheduler(optimizer, 512)\n","\n","# Epoch 값 설정\n","epochs = 10\n","for epoch in range(epochs):\n","    train_loss = train(train_dataloader)\n","    print(f'Epoch {epoch+1}, Train loss: {train_loss:.4f}')\n"],"metadata":{"id":"gW9XgsZS25bo","scrolled":true,"execution":{"iopub.status.busy":"2024-07-30T06:36:38.363663Z","iopub.execute_input":"2024-07-30T06:36:38.364304Z","iopub.status.idle":"2024-07-30T07:35:22.237732Z","shell.execute_reply.started":"2024-07-30T06:36:38.364273Z","shell.execute_reply":"2024-07-30T07:35:22.236747Z"},"trusted":true,"outputId":"5e89f8f5-c2a7-4dbe-ae48-d411b1c4b4c6"},"execution_count":null,"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:55<00:00,  1.06it/s, loss=0.607]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train loss: 0.6528\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:54<00:00,  1.06it/s, loss=0.49] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train loss: 0.4813\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:54<00:00,  1.06it/s, loss=0.56] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train loss: 0.4107\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:53<00:00,  1.06it/s, loss=0.447]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Train loss: 0.3556\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:52<00:00,  1.06it/s, loss=0.586]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Train loss: 0.3153\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:51<00:00,  1.07it/s, loss=0.55] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train loss: 0.2775\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:50<00:00,  1.07it/s, loss=0.68]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Train loss: 0.2608\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:50<00:00,  1.07it/s, loss=0.415] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Train loss: 0.2493\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:50<00:00,  1.07it/s, loss=0.348] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Train loss: 0.2172\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 375/375 [05:49<00:00,  1.07it/s, loss=0.296] ","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train loss: 0.2103\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Evaluation 결과 accuracy가 0.8 이상 나오면 통과입니다!"],"metadata":{"id":"NwZJcUGMk94k"}},{"cell_type":"code","source":["def evaluate(dataloader):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # No gradients needed\n","        progress_bar = tqdm(dataloader, desc=\"Evaluation\", leave=True)\n","        for label, text in progress_bar:\n","            label, text = label.to(device), text.to(device)\n","            src_padding_mask = create_padding_mask_for_attention(text, PAD_IDX)\n","\n","            output = model(text, src_padding_mask)\n","            loss = criterion(output, label)\n","            total_loss += loss.item()\n","\n","            # Convert model output to predicted labels\n","            # Assuming binary classification with a sigmoid output layer\n","            prediction = (torch.sigmoid(output) > 0.5).float()\n","            correct += (prediction == label).sum().item()\n","            total += label.size(0)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","\n","# Assuming model, criterion, and device are defined\n","test_loss, test_accuracy = evaluate(test_dataloader)\n","print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112187,"status":"ok","timestamp":1722271427690,"user":{"displayName":"김민준","userId":"00376366833054431661"},"user_tz":-540},"id":"snwZ8nIz-2Ib","outputId":"9856048a-d3b8-4f37-e5ae-31687c5d3393","scrolled":true,"execution":{"iopub.status.busy":"2024-07-30T07:37:56.904684Z","iopub.execute_input":"2024-07-30T07:37:56.905072Z","iopub.status.idle":"2024-07-30T07:40:09.886746Z","shell.execute_reply.started":"2024-07-30T07:37:56.905032Z","shell.execute_reply":"2024-07-30T07:40:09.885838Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Evaluation: 100%|██████████| 375/375 [02:12<00:00,  2.82it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.8360\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}